{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5.1 Modeling Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data preprocessing for modeling text sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use IMDB review data set to train a Recurrent Neural Networks (RNN) model, by using two (2) type of text sequences as model input: characters and words. Data can be downloaded from https://storage.googleapis.com/trl_data/imdb_dataset.zip. Training set contains 25000 reviews with labels 0 for \"negative\" sentiment and 1 for \"positive\" sentiment. For validation set, the information about binary labels (0 and 1) can be seen in attribute \"id\" of the data set. Number after character '\\_' represents rating score. If rating <5, then the sentiment score is 0 or \"negative\" sentiment. If the rating is greater than 7, then the score is 1 or \"positive\". Otherwise, it is negative (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of (part of) original text in data set:\n",
    "\n",
    "```\n",
    "id\tsentiment\treview\n",
    "\n",
    "\"7759_3\"\t0\t\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger .\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a text (e.g. a movie review), we need to predict whether this review is positive (class label = 1) or negative (class label = 0). We will work with two (2) types of preprocessing to create sequence for our model input: character-level and word-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data preprocessing for text sequence:\n",
    "\n",
    "* Cleaning raw text data\n",
    "    - remove HTML tags\n",
    "    - remove non-informative characters\n",
    "* Tokenizing raw text into array of word tokens (for word-level sequences)\n",
    "* Create vocabulary index: character based and word based look up dictionary index\n",
    "* Transform tokenized text into integer sequences (based on look up vocabulary index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "DATA_PATH = 'amazon_data'\n",
    "EMBEDDING_PATH = 'embedding'\n",
    "MODEL_PATH = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create above directories under your current working directory. Download data set provided and locate it in directory 'data' above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean raw text data\n",
    "\n",
    "def striphtml(html):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', html)\n",
    "\n",
    "def clean(s):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_labelled_data = pd.read_csv(os.path.join(DATA_PATH,\"example1_labelled.tsv\"), header=0, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_unlabelled_data = pd.read_csv(os.path.join(DATA_PATH,\"example2_unlabelled.tsv\"), header=0, delimiter=\"\\t\")\n",
    "ex2_labelled_data = pd.read_csv(os.path.join(DATA_PATH,\"example2_labelled.tsv\"), header=0, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>camera</td>\n",
       "      <td>My husband bought this camera about 3 months ago and we continue to love it...wow, what an impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>laptop</td>\n",
       "      <td>I got this notebook several months ago and I've had a great experience with it. I've had zero pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mobilephone</td>\n",
       "      <td>I have this phone for about 10 months.  The calls are clear in many places where I can't get my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "0       camera   \n",
       "1       laptop   \n",
       "2  mobilephone   \n",
       "\n",
       "                                                                                                review  \n",
       "0  My husband bought this camera about 3 months ago and we continue to love it...wow, what an impro...  \n",
       "1  I got this notebook several months ago and I've had a great experience with it. I've had zero pr...  \n",
       "2  I have this phone for about 10 months.  The calls are clear in many places where I can't get my ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1_labelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I purchased the 20d in Feb 2011, around 7 years after it was first introduced. The camera was so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's been 3 weeks now and I've only had minor voice mail setup issues.  The phone itself is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I purchased this Z Series laptop about 5 months ago, its my first Sony laptop, Im a Mac fan, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When I first got this laptop (at a garage sale, broken) It was really slow and leggy so I instal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love this phone. I've had my own for over a year now and have since bought one for my son and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I purchased this camera to replace my Casio EX-Z4 which broke.  This camera takes great photos. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My perfect camera has to do two things very well.  First, it has to deliver superior results.  S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                review\n",
       "0  I purchased the 20d in Feb 2011, around 7 years after it was first introduced. The camera was so...\n",
       "1  It's been 3 weeks now and I've only had minor voice mail setup issues.  The phone itself is the ...\n",
       "2  I purchased this Z Series laptop about 5 months ago, its my first Sony laptop, Im a Mac fan, but...\n",
       "3  When I first got this laptop (at a garage sale, broken) It was really slow and leggy so I instal...\n",
       "4  I love this phone. I've had my own for over a year now and have since bought one for my son and ...\n",
       "5  I purchased this camera to replace my Casio EX-Z4 which broke.  This camera takes great photos. ...\n",
       "6  My perfect camera has to do two things very well.  First, it has to deliver superior results.  S..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex2_unlabelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>camera</td>\n",
       "      <td>I purchased the 20d in Feb 2011, around 7 years after it was first introduced. The camera was so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mobilephone</td>\n",
       "      <td>It's been 3 weeks now and I've only had minor voice mail setup issues.  The phone itself is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>laptop</td>\n",
       "      <td>I purchased this Z Series laptop about 5 months ago, its my first Sony laptop, Im a Mac fan, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>laptop</td>\n",
       "      <td>When I first got this laptop (at a garage sale, broken) It was really slow and leggy so I instal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mobilephone</td>\n",
       "      <td>I love this phone. I've had my own for over a year now and have since bought one for my son and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>camera</td>\n",
       "      <td>I purchased this camera to replace my Casio EX-Z4 which broke.  This camera takes great photos. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>camera</td>\n",
       "      <td>My perfect camera has to do two things very well.  First, it has to deliver superior results.  S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  \\\n",
       "0       camera   \n",
       "1  mobilephone   \n",
       "2       laptop   \n",
       "3       laptop   \n",
       "4  mobilephone   \n",
       "5       camera   \n",
       "6       camera   \n",
       "\n",
       "                                                                                                review  \n",
       "0  I purchased the 20d in Feb 2011, around 7 years after it was first introduced. The camera was so...  \n",
       "1  It's been 3 weeks now and I've only had minor voice mail setup issues.  The phone itself is the ...  \n",
       "2  I purchased this Z Series laptop about 5 months ago, its my first Sony laptop, Im a Mac fan, but...  \n",
       "3  When I first got this laptop (at a garage sale, broken) It was really slow and leggy so I instal...  \n",
       "4  I love this phone. I've had my own for over a year now and have since bought one for my son and ...  \n",
       "5  I purchased this camera to replace my Casio EX-Z4 which broke.  This camera takes great photos. ...  \n",
       "6  My perfect camera has to do two things very well.  First, it has to deliver superior results.  S...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex2_labelled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this  will create a cleaned version of training set\n",
    "\n",
    "train_docs = []\n",
    "train_labels = []\n",
    "for cont, label in zip(ex1_labelled_data.review, ex1_labelled_data.label):\n",
    "    \n",
    "    doc = clean(striphtml(cont))\n",
    "    doc = doc.lower() \n",
    "    train_docs.append(doc)\n",
    "    train_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this  will create a cleaned version of validation set\n",
    "# we also need to extract labels from attribute 'id'\n",
    "\n",
    "valid_docs =[]\n",
    "valid_labels = []\n",
    "for cont, label in zip(ex2_labelled_data.review, ex2_labelled_data.label):\n",
    "    \n",
    "    doc = clean(striphtml(cont))\n",
    "    doc = doc.lower() \n",
    "    valid_docs.append(doc)\n",
    "    valid_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Build vocabulary index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION to tokenize documents into array list of words\n",
    "# you may also use nltk tokenizer, sklearn tokenizer, or keras tokenizer - \n",
    "# but for the tutorial in text modeling, we will use below function: \n",
    "\n",
    "def tokenizeWords(text):\n",
    "    \n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).split()\n",
    "    return [str(strtokens) for strtokens in tokens]\n",
    "\n",
    "# FUNCTION to create word-level vocabulary index\n",
    "\n",
    "def indexingVocabulary(array_of_words):\n",
    "\n",
    "    wordIndex = list(array_of_words)\n",
    "    \n",
    "    # we will later pad our sequence into fixed length, so\n",
    "    # we will use '0' as the integer index of pad \n",
    "    wordIndex.insert(0,'<pad>')\n",
    "    \n",
    "    # index for word token '<start>' as a starting sign of sequence. We won't use it for this model\n",
    "    # but for the latter model (sequence-to-sequence model)\n",
    "    wordIndex.append('<start>')\n",
    "    \n",
    "    # index for word token '<end>' as an ending sign of sequence. We won't use it for this model\n",
    "    # but for the latter model (sequence-to-sequence model)\n",
    "    wordIndex.append('<end>')\n",
    "    \n",
    "    # index for word token '<unk>' or unknown words (out of vocabulary words) \n",
    "    wordIndex.append('<unk>')\n",
    "    \n",
    "    vocab=dict([(i,wordIndex[i]) for i in range(len(wordIndex))])\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization (for word sequences as model input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create array list of tokenized words and merged array of these word tokens to generate vocabulary index. Notice that we only use 10.000 most frequent words from training set. Out of Vocabulary (OOV) words will be presented as '<unk>' or unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text from training set\n",
    "\n",
    "train_str_tokens = []\n",
    "all_tokens = []\n",
    "for i, text in enumerate(train_docs):\n",
    "    \n",
    "    # this will create our training corpus\n",
    "    train_str_tokens.append(tokenizeWords(text))\n",
    "    \n",
    "    # this will be our merged array to create vocabulary index\n",
    "    all_tokens.extend(tokenizeWords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likewise, tokenize text from validation set\n",
    "\n",
    "valid_str_tokens = []\n",
    "for i, text in enumerate(valid_docs):\n",
    "\n",
    "    valid_str_tokens.append(tokenizeWords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nltk to count word frequency and use 10.000 most frequent words to generate vocabulary index\n",
    "\n",
    "tf = nltk.FreqDist(all_tokens)\n",
    "common_words = tf.most_common(10000)\n",
    "arr_common = np.array(common_words)\n",
    "words = arr_common[:,0]\n",
    "\n",
    "# create vocabulary index\n",
    "\n",
    "# word- index pairs\n",
    "words_indices = indexingVocabulary(words)\n",
    "\n",
    "# index - word pairs\n",
    "indices_words = dict((v,k) for (k,v) in words_indices.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '<pad>'), (1, 'i'), (2, 'it'), (3, 'and'), (4, 'a')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(words_indices.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<pad>', 0), ('i', 1), ('it', 2), ('and', 3), ('a', 4)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(indices_words.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocabulary index\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'words_indices.npy'), words_indices)\n",
    "np.save(os.path.join(DATA_PATH,'indices_words.npy'), indices_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Preparing model input - output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# integer format of training input \n",
    "train_int_input = []\n",
    "for i, text in enumerate(train_str_tokens):\n",
    "    int_tokens = [indices_words[w] if w in indices_words.keys() else indices_words['<unk>'] for w in text ]\n",
    "    train_int_input.append(int_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer format of test validation input \n",
    "valid_int_input = []\n",
    "for i, text in enumerate(valid_str_tokens):\n",
    "    int_tokens = [indices_words[w] if w in indices_words.keys() else indices_words['<unk>'] for w in text ]\n",
    "    valid_int_input.append(int_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr = np.array(train_int_input)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "X_valid_arr = np.array(valid_int_input)\n",
    "y_valid = np.array(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding word sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define maximum 500 words as our fixed length of input sequences. Here, we use keras padding, but you may also define your own padding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s134277\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train_arr, maxlen=max_review_length)\n",
    "X_valid = sequence.pad_sequences(X_valid_arr, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'X_train_word.npy'), X_train)\n",
    "np.save(os.path.join(DATA_PATH,'y_train_word.npy'), y_train)\n",
    "\n",
    "np.save(os.path.join(DATA_PATH,'X_valid_word.npy'), X_valid)\n",
    "np.save(os.path.join(DATA_PATH,'y_valid_word.npy'), y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
