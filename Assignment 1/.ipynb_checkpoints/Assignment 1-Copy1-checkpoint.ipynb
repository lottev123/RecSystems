{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s134277\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\s134277\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0, 46, 71, ...,  1,  0,  0],\n",
       "       [ 0, 71, 18, ...,  1,  1,  1],\n",
       "       ...,\n",
       "       [ 0,  1,  1, ...,  0,  0,  0],\n",
       "       [ 0,  0,  1, ...,  0,  0,  0],\n",
       "       [ 0,  0,  1, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create co-occurrence matrix\n",
    "shape = (V, V)\n",
    "matrix = np.zeros(shape, dtype = int)\n",
    "for sentence in corpus:\n",
    "    for i in range(0, len(sentence)):\n",
    "        coword = sentence[i+1:i+4+1]\n",
    "        for j in coword:\n",
    "            matrix[sentence[i], j] = matrix[sentence[i], j] + 1\n",
    "matrix_trans = matrix.transpose()\n",
    "co_occurence_matrix = np.add(matrix, matrix_trans)\n",
    "co_occurence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Cosine similarity Alice - Dinah: 0.0000\n",
      "Cosine similarity Alice - Rabbit: 0.0000\n",
      "Cosine similarity Dinah - Rabbit: 0.0000\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#normalize the words by dividing the co-occurence frequency with the total frequency of one word\n",
    "for i in range(1, V):\n",
    "    total_tf = np.sum(co_occurence_matrix[:,i])\n",
    "    if total_tf > 0: #some words do not co-occur with other words, as they are the only words in the sentence (for instance 797)\n",
    "        for j in range(1,V):\n",
    "            co_occurence_matrix[j,i] = co_occurence_matrix[j,i]/total_tf\n",
    "\n",
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "alice_index = tokenizer.word_index['alice']\n",
    "dinah_index = tokenizer.word_index['dinah']\n",
    "rabit_index = tokenizer.word_index['rabbit']\n",
    "#create similarity matrix\n",
    "sim = cosine_similarity(co_occurence_matrix)\n",
    "sim_alice_dinah = sim[alice_index, dinah_index]\n",
    "sim_alice_rabit = sim[alice_index, rabit_index]\n",
    "sim_dinah_rabit = sim[dinah_index, rabit_index]\n",
    "print(sim_alice_dinah)\n",
    "print(sim_alice_rabit)\n",
    "print(sim_dinah_rabit)\n",
    "print(\"Cosine similarity Alice - Dinah: %.4f\" %sim_alice_dinah)\n",
    "print(\"Cosine similarity Alice - Rabbit: %.4f\" %sim_alice_rabit)\n",
    "print(\"Cosine similarity Dinah - Rabbit: %.4f\" %sim_dinah_rabit)\n",
    "\n",
    "print(co_occurence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 589    0  292   68  143 1182]\n",
      "here\n",
      "ve\n",
      "old\n",
      "conversations\n",
      "pairs\n"
     ]
    }
   ],
   "source": [
    "#find the closest words to Alice\n",
    "nbrs = nn(n_neighbors=6, algorithm='auto').fit(co_occurence_matrix)\n",
    "distances, indices = nbrs.kneighbors(co_occurence_matrix)\n",
    "\n",
    "print(indices[alice_index])\n",
    "for word, index in tokenizer.word_index.items():    # for name, age in list.items():  (for Python 3.x)\n",
    "    if index in indices[alice_index]:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice was beginning to get very tired of sitting by her sister on the\\n',\n",
       " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\",\n",
       " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n',\n",
       " 'Alice started to her feet, for it flashed across her mind that she had\\n',\n",
       " 'In another moment down went Alice after it, never once considering how\\n',\n",
       " 'dipped suddenly down, so suddenly that Alice had not a moment to think\\n',\n",
       " \"'Well!' thought Alice to herself, 'after such a fall as this, I shall\\n\",\n",
       " \"thousand miles down, I think--' (for, you see, Alice had learnt several\\n\",\n",
       " \"or Longitude I've got to?' (Alice had no idea what Latitude was, or\\n\",\n",
       " 'Down, down, down. There was nothing else to do, so Alice soon began\\n',\n",
       " \"like a mouse, you know. But do cats eat bats, I wonder?' And here Alice\\n\",\n",
       " 'Alice was not a bit hurt, and she jumped up on to her feet in a moment:\\n',\n",
       " 'There was not a moment to be lost: away went Alice like the wind, and\\n',\n",
       " 'Alice had been all the way down one side and up the other, trying every\\n',\n",
       " \"glass; there was nothing on it except a tiny golden key, and Alice's\\n\",\n",
       " 'Alice opened the door and found that it led into a small passage, not\\n',\n",
       " \"doorway; 'and even if my head would go through,' thought poor Alice, 'it\\n\",\n",
       " 'that Alice had begun to think that very few things indeed were really\\n',\n",
       " \"before,' said Alice,) and round the neck of the bottle was a paper\\n\",\n",
       " \"It was all very well to say 'Drink me,' but the wise little Alice was\\n\",\n",
       " \"However, this bottle was NOT marked 'poison,' so Alice ventured to taste\\n\",\n",
       " \"'What a curious feeling!' said Alice; 'I must be shutting up like a\\n\",\n",
       " \"Alice to herself, 'in my going out altogether, like a candle. I wonder\\n\",\n",
       " 'into the garden at once; but, alas for poor Alice! when she got to the\\n',\n",
       " \"'Come, there's no use in crying like that!' said Alice to herself,\\n\",\n",
       " \"'But it's no use now,' thought poor Alice, 'to pretend to be two people!\\n\",\n",
       " \"Alice, 'and if it makes me grow larger, I can reach the key; and if it\\n\",\n",
       " 'size: to be sure, this generally happens when one eats cake, but Alice\\n',\n",
       " \"'Curiouser and curiouser!' cried Alice (she was so much surprised, that\\n\",\n",
       " \"kind to them,' thought Alice, 'or perhaps they won't walk the way I want\\n\",\n",
       " 'Poor Alice! It was as much as she could do, lying down on one side, to\\n',\n",
       " \"'You ought to be ashamed of yourself,' said Alice, 'a great girl like\\n\",\n",
       " \"Oh! won't she be savage if I've kept her waiting!' Alice felt so\\n\",\n",
       " 'Alice took up the fan and gloves, and, as the hall was very hot, she\\n',\n",
       " \"'I'm sure those are not the right words,' said poor Alice, and her eyes\\n\",\n",
       " 'till I\\'m somebody else\"--but, oh dear!\\' cried Alice, with a sudden burst\\n',\n",
       " \"'That WAS a narrow escape!' said Alice, a good deal frightened at the\\n\",\n",
       " \"railway,' she said to herself. (Alice had been to the seaside once in\\n\",\n",
       " \"'I wish I hadn't cried so much!' said Alice, as she swam about, trying\\n\",\n",
       " \"'Would it be of any use, now,' thought Alice, 'to speak to this mouse?\\n\",\n",
       " \"of swimming about here, O Mouse!' (Alice thought this must be the right\\n\",\n",
       " \"'Perhaps it doesn't understand English,' thought Alice; 'I daresay it's\\n\",\n",
       " 'her knowledge of history, Alice had no very clear notion how long ago\\n',\n",
       " \"'Oh, I beg your pardon!' cried Alice hastily, afraid that she had hurt\\n\",\n",
       " \"'Well, perhaps not,' said Alice in a soothing tone: 'don't be angry\\n\",\n",
       " \"thing,' Alice went on, half to herself, as she swam lazily about in the\\n\",\n",
       " 'Alice again, for this time the Mouse was bristling all over, and she\\n',\n",
       " \"'I won't indeed!' said Alice, in a great hurry to change the subject of\\n\",\n",
       " \"answer, so Alice went on eagerly: 'There is such a nice little dog near\\n\",\n",
       " \"says it kills all the rats and--oh dear!' cried Alice in a sorrowful\\n\",\n",
       " 'face was quite pale (with passion, Alice thought), and it said in a low\\n',\n",
       " 'a Lory and an Eaglet, and several other curious creatures. Alice led the\\n',\n",
       " 'to Alice to find herself talking familiarly with them, as if she had\\n',\n",
       " \"you, and must know better'; and this Alice would not allow without\\n\",\n",
       " 'in the middle. Alice kept her eyes anxiously fixed on it, for she felt\\n',\n",
       " 'to Alice as it spoke.\\n',\n",
       " \"'As wet as ever,' said Alice in a melancholy tone: 'it doesn't seem to\\n\",\n",
       " \"'What IS a Caucus-race?' said Alice; not that she wanted much to know,\\n\",\n",
       " \"'Why, SHE, of course,' said the Dodo, pointing to Alice with one finger;\\n\",\n",
       " 'Alice had no idea what to do, and in despair she put her hand in her\\n',\n",
       " \"your pocket?' he went on, turning to Alice.\\n\",\n",
       " \"'Only a thimble,' said Alice sadly.\\n\",\n",
       " 'Alice thought the whole thing very absurd, but they all looked so grave\\n',\n",
       " \"'You promised to tell me your history, you know,' said Alice, 'and why\\n\",\n",
       " \"'Mine is a long and a sad tale!' said the Mouse, turning to Alice, and\\n\",\n",
       " \"'It IS a long tail, certainly,' said Alice, looking down with wonder at\\n\",\n",
       " \"'You are not attending!' said the Mouse to Alice severely. 'What are you\\n\",\n",
       " \"'I beg your pardon,' said Alice very humbly: 'you had got to the fifth\\n\",\n",
       " \"'A knot!' said Alice, always ready to make herself useful, and looking\\n\",\n",
       " \"'I didn't mean it!' pleaded poor Alice. 'But you're so easily offended,\\n\",\n",
       " \"'Please come back and finish your story!' Alice called after it; and the\\n\",\n",
       " \"'I wish I had our Dinah here, I know I do!' said Alice aloud, addressing\\n\",\n",
       " 'Alice replied eagerly, for she was always ready to talk about her pet:\\n',\n",
       " 'On various pretexts they all moved off, and Alice was soon left alone.\\n',\n",
       " \"any more!' And here poor Alice began to cry again, for she felt very\\n\",\n",
       " \"ferrets! Where CAN I have dropped them, I wonder?' Alice guessed in a\\n\",\n",
       " 'Very soon the Rabbit noticed Alice, as she went hunting about, and\\n',\n",
       " \"Quick, now!' And Alice was so much frightened that she ran off at once\\n\",\n",
       " \"'How queer it seems,' Alice said to herself, 'to be going messages for\\n\",\n",
       " 'began fancying the sort of thing that would happen: \\'\"Miss Alice! Come\\n',\n",
       " \"think,' Alice went on, 'that they'd let Dinah stop in the house if it\\n\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_text = open(path).readlines()[0:700]\n",
    "corpus_text = [sentence for sentence in corpus_text if sentence.count(\" \") >= 2]\n",
    "corpus_text_alice = [sentence for sentence in corpus_text if \"Alice\" in sentence]\n",
    "corpus_text_alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999985709069524"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage of cells with zeros\n",
    "np.where(co_occurence_matrix==0)[0].size/co_occurence_matrix.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawbacks:\n",
    "- Most of the co-occurence matrix is very sparse. In this data set, 98,16% of the cells in the word-word co-occurence matrix are zeros. \n",
    "- The matrices tend to be very large. For the first 700 lines of the book Alice in Wonderland, we already need a matrix of size 1183x1183. \n",
    "- The frequency of occurence can be skewed and non-discriminative. Words like 'the', 'and' and 'to' are not very discriminative, but the impact on the calculation of word similarity of these words is equally big as the impact of more discriminative words. However, this is not necessary a drawback of the use of the word-word co-occurence matrix, as one can adjust for this by using weighting terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cd3170d986ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = []\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reopen your file as follows\n",
    "\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for cbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for Skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train Skipgram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CBOW model with additional dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model for CBOW + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Skipgram with additional dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function for Skipgram + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model for Skipgram + dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"your path /GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
