{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niek\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Niek\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()[0:700]\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Is this something they need to change?\n",
    "dim = 100\n",
    "window_size = 2 #use this window size for Skipgram, CBOW, and the model with the additional hidden layer\n",
    "window_size_corpus = 4 #use this window size for the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "Use the provided code to load the \"Alice in Wonderland\" text document. \n",
    "1. Implement the word-word co-occurrence matrix for “Alice in Wonderland”\n",
    "2. Normalize the words such that every value lies within a range of 0 and 1\n",
    "3. Compute the cosine distance between the given words:\n",
    "    - Alice \n",
    "    - Dinah\n",
    "    - Rabbit\n",
    "4. List the 5 closest words to 'Alice'. Discuss the results.\n",
    "5. Discuss what the main drawbacks are of a term-term co-occurence matrix solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create co-occurrence matrix\n",
    "shape = (V, V)\n",
    "matrix = np.zeros(shape)\n",
    "np.fill_diagonal(matrix, 0)\n",
    "for sentence in corpus:\n",
    "    for i in range(0, len(sentence)):\n",
    "        coword = sentence[i+1:i+window_size_corpus+1]\n",
    "        for j in coword:\n",
    "            matrix[sentence[i], j] = matrix[sentence[i], j] + 1\n",
    "matrix_trans = matrix.transpose()\n",
    "co_occurence_matrix = np.add(matrix, matrix_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the words by dividing the co-occurence frequency with the total frequency of one word\n",
    "for i in range(1, V):\n",
    "    total_tf = np.sum(co_occurence_matrix[:,i])\n",
    "    if total_tf > 0: #some words do not co-occur with other words, as they are the only words in the sentence (for instance 797)\n",
    "        for j in range(1,V):\n",
    "            co_occurence_matrix[j,i] = co_occurence_matrix[j,i]/total_tf\n",
    "\n",
    "#find cosine similarity to Alice, Dinah and Rabbit\n",
    "alice_index = tokenizer.word_index['alice']\n",
    "dinah_index = tokenizer.word_index['dinah']\n",
    "rabit_index = tokenizer.word_index['rabbit']\n",
    "#create similarity matrix\n",
    "sim = cosine_similarity(co_occurence_matrix)\n",
    "sim_alice_dinah = sim[alice_index, dinah_index]\n",
    "sim_alice_rabit = sim[alice_index, rabit_index]\n",
    "sim_dinah_rabit = sim[dinah_index, rabit_index]\n",
    "print(\"Cosine similarity Alice - Dinah: %.4f\" %sim_alice_dinah)\n",
    "print(\"Cosine similarity Alice - Rabbit: %.4f\" %sim_alice_rabit)\n",
    "print(\"Cosine similarity Dinah - Rabbit: %.4f\" %sim_dinah_rabit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the closest words to Alice\n",
    "nbrs = nn(n_neighbors=6, algorithm='auto').fit(co_occurence_matrix)\n",
    "distances, indices = nbrs.kneighbors(co_occurence_matrix)\n",
    "\n",
    "print(indices[alice_index])\n",
    "for word, index in tokenizer.word_index.items():    # for name, age in list.items():  (for Python 3.x)\n",
    "    if index in indices[alice_index]:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The length of the co-occurence matrix is {}'.format(len(co_occurence_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trivially, the closest word to Alice is Alice. However, all other works are not in line with what one would expect. Having read the first lines of Alice in Wonderland, it is for example not clear what the connection is between 'Alice' and 'foot'. A possible explanation is that the dimensionality of the data is high, as the co-occurency of two terms is compared with all (1182-2=)1180. Because of this high dimensionality, the distance difference between the nearest and farthest neighbor is low. To illustrate this, we computed the 100 nearest neighbours to 'Alice' below and compared the longest of these 100 distances with the smallest distance bigger than zero. As can be seen in the output of the code below, the nearest neighbour is only 1.9% closer in terms of distance than the 99th nearest neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs_100 = nn(n_neighbors=1000, algorithm='auto').fit(co_occurence_matrix)\n",
    "distances_1000, indices_1000 = nbrs.kneighbors(co_occurence_matrix)\n",
    "\n",
    "print(distances[alice_index]/np.average(distances_1000[alice_index]))\n",
    "print(distances[alice_index]/np.max(distances_1000[alice_index]))\n",
    "np.max(np.max(distances_1000[alice_index])/distances[alice_index][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of cells with zeros\n",
    "print('The ratio of cells with zeros is {:.4f}'.format\n",
    "      (np.where(co_occurence_matrix==0)[0].size/co_occurence_matrix.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the drawback of a term-term co-occurence matrix:\n",
    "- A co-occurence matrix is very sparse. In this data set, 98,16% of the cells in the word-word co-occurence matrix are zeros. \n",
    "- The matrix tends to be very large. For the first 700 lines of the book Alice in Wonderland, we already need a matrix of size 1183x1183. \n",
    "- The frequency of occurence can be skewed and non-discriminative. Words like 'the', 'and' and 'to' are not very discriminative, but the impact on the calculation of word similarity of these words is equally big as the impact of more discriminative words. However, this is not necessary a drawback of the use of the word-word co-occurence matrix, as one can adjust for this by using weighting terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your all the vector representations of your word embeddings in this way\n",
    "#Change when necessary the sizes of the vocabulary/embedding dimension\n",
    "\n",
    "f = open('vectors_co_occurrence.txt',\"w\")\n",
    "f.write(\" \".join([str(V-1),str(V-1)]))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "#vectors = your word co-occurrence matrix\n",
    "vectors = [co_occurence_matrix]\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    #Dit werkt wel, maar dan krijg je later weer problemen\n",
    "#     f.write(\" \".join(map(str, list(vectors[0][i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "#reopen your file as follows\n",
    "co_occurrence = KeyedVectors.load_word2vec_format('./vectors_co_occurrence.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the CBOW model\n",
    "2. Using Skipgram model\n",
    "3. Add extra hidden dense layer to CBow and Skipgram implementations. Choose an activation function for that layer and justify your answer.\n",
    "4. Analyze the four different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings that you have trained in 2.1, 2.2 and 2.3.  \n",
    "    - Visualize your results and interpret your results\n",
    "5. Use the word co-occurence matrix from Question 1. Compare the performance on the analogy task with the performance of your trained word embeddings.  \n",
    "6. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What is the advantage of negative sampling?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n",
    "7. Load pre-trained embeddings on large corpuses (see the pdf file). You only have to consider the word embeddings with an embedding size of 300\n",
    "    - Compare performance on the analogy task with your own trained embeddings from \"Alice in Wonderland\". You can limit yourself to the vocabulary of Alice in Wonderland. Visualize the pre-trained word embeddings and compare these with the results of your own trained word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for cbow|\n",
    "def gen(corpus, window_size, V):\n",
    "    maxlen = window_size * 2\n",
    "    for sentence in corpus:\n",
    "        L = len(sentence)\n",
    "        for index, word in enumerate(sentence):\n",
    "            context = []\n",
    "            labels = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            #Add context word (cat is hungry) if  index is 1 then context is cat and hungry\n",
    "            context.append([sentence[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(context, maxlen=maxlen)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            #return generator.\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41686.85704231262\n",
      "1 39117.07442855835\n",
      "2 38909.00557476282\n",
      "3 38815.762184262276\n",
      "4 38761.799980700016\n",
      "5 38724.25989371538\n",
      "6 38676.12575338781\n",
      "7 38627.986487567425\n",
      "8 38588.05231449008\n",
      "9 38562.03263479099\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "cbow_model_50 = Sequential()\n",
    "cbow_model_50.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow_model_50.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow_model_50.add(Dense(V, activation='softmax'))\n",
    "#Compile\n",
    "cbow_model_50.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "#Train CBOW Model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in gen(corpus, window_size, V):\n",
    "        loss += cbow_model_50.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41624.23516559601\n",
      "1 38758.47251689434\n",
      "2 38354.17740005255\n",
      "3 38128.833682626486\n",
      "4 37915.97992122173\n",
      "5 37717.73056745529\n",
      "6 37544.31483665854\n",
      "7 37407.15357390046\n",
      "8 37298.29355230555\n",
      "9 37203.6013012426\n"
     ]
    }
   ],
   "source": [
    "dim = 150\n",
    "cbow_model_150 = Sequential()\n",
    "cbow_model_150.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow_model_150.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow_model_150.add(Dense(V, activation='softmax'))\n",
    "#Compile\n",
    "cbow_model_150.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "#Train CBOW Model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in gen(corpus, window_size, V):\n",
    "        loss += cbow_model_150.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41559.90415096283\n",
      "1 38492.521023631096\n",
      "2 37942.09507280588\n",
      "3 37626.64058974385\n",
      "4 37343.293199047446\n",
      "5 37085.93880838156\n",
      "6 36874.7218532674\n",
      "7 36709.47696916293\n",
      "8 36572.27346193092\n",
      "9 36450.24198226421\n"
     ]
    }
   ],
   "source": [
    "dim = 300\n",
    "cbow_model_300 = Sequential()\n",
    "cbow_model_300.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow_model_300.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow_model_300.add(Dense(V, activation='softmax'))\n",
    "#Compile\n",
    "cbow_model_300.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "#Train CBOW Model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in gen(corpus, window_size, V):\n",
    "        loss += cbow_model_300.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "cbow_model_50_extra_layer = Sequential()\n",
    "cbow_model_50_extra_layer.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow_model_50_extra_layer.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow_model_50_extra_layer.add(Dense(V, activation='selu'))\n",
    "cbow_model_50_extra_layer.add(Dense(V, activation='softmax'))\n",
    "#Compile\n",
    "cbow_model_50_extra_layer.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "#Train CBOW Model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in gen(corpus, window_size, V):\n",
    "        loss += cbow_model_50_extra_layer.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 150\n",
    "cbow_model_150_extra_layer = Sequential()\n",
    "cbow_model_150_extra_layer.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow_model_150_extra_layer.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow_model_150_extra_layer.add(Dense(V, activation='selu'))\n",
    "cbow_model_150_extra_layer.add(Dense(V, activation='softmax'))\n",
    "#Compile\n",
    "cbow_model_150_extra_layer.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "#Train CBOW Model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in gen(corpus, window_size, V):\n",
    "        loss += cbow_model_150_extra_layer.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 150\n",
    "cbow_model_300_extra_layer = Sequential()\n",
    "cbow_model_300_extra_layer.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow_model_300_extra_layer.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow_model_300_extra_layer.add(Dense(V, activation='selu'))\n",
    "cbow_model_300_extra_layer.add(Dense(V, activation='softmax'))\n",
    "#Compile\n",
    "cbow_model_300_extra_layer.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "#Train CBOW Model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in gen(corpus, window_size, V):\n",
    "        loss += cbow_model_300_extra_layer.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write results\n",
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim))\n",
    "#Write results\n",
    "cbow_model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data for Skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    for sentence in corpus:\n",
    "        L = len(sentence)\n",
    "        for index, word in enumerate(sentence):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word])\n",
    "                    labels.append(sentence[i])\n",
    "            if in_words != []:\n",
    "                x = np.array(in_words,dtype=np.int32)\n",
    "                y = np_utils.to_categorical(labels, V)\n",
    "                yield (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41271.83740758896\n",
      "1 39101.409803152084\n",
      "2 39265.024810791016\n",
      "3 39357.28198981285\n",
      "4 39443.99115431309\n",
      "5 39536.58359324932\n",
      "6 39641.83396399021\n",
      "7 39760.61421716213\n",
      "8 39890.63317453861\n",
      "9 40026.53615760803\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "#create Skipgram model\n",
    "skipgram_50 = Sequential()\n",
    "skipgram_50.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_50.add(Reshape((dim, )))\n",
    "skipgram_50.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "#define loss function for Skipgram\n",
    "skipgram_50.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram(corpus,window_size,V):\n",
    "        loss += skipgram_50.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41215.39843392372\n",
      "1 38922.62371468544\n",
      "2 38984.99028789997\n",
      "3 39005.47671818733\n",
      "4 39030.06290924549\n",
      "5 39068.809734106064\n",
      "6 39119.5119754076\n",
      "7 39171.81856572628\n",
      "8 39215.354902505875\n",
      "9 39248.874417066574\n"
     ]
    }
   ],
   "source": [
    "dim = 150\n",
    "#create Skipgram model\n",
    "skipgram_150 = Sequential()\n",
    "skipgram_150.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_150.add(Reshape((dim, )))\n",
    "skipgram_150.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "#define loss function for Skipgram\n",
    "skipgram_150.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram(corpus,window_size,V):\n",
    "        loss += skipgram_150.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41158.67156767845\n",
      "1 38737.37768530846\n",
      "2 38703.30988729\n",
      "3 38658.188072800636\n",
      "4 38628.11664748192\n",
      "5 38608.34821033478\n",
      "6 38582.74828064442\n",
      "7 38541.39902639389\n",
      "8 38489.80666589737\n",
      "9 38436.1510027647\n"
     ]
    }
   ],
   "source": [
    "dim = 300\n",
    "#create Skipgram model\n",
    "skipgram_300 = Sequential()\n",
    "skipgram_300.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_300.add(Reshape((dim, )))\n",
    "skipgram_300.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "#define loss function for Skipgram\n",
    "skipgram_300.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram(corpus,window_size,V):\n",
    "        loss += skipgram_300.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "#create Skipgram model\n",
    "skipgram_50_extra_layer = Sequential()\n",
    "skipgram_50_extra_layer.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_50_extra_layer.add(Reshape((dim, )))\n",
    "skipgram_50_extra_layer.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='selu'))\n",
    "skipgram_50_extra_layer.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "#define loss function for Skipgram\n",
    "skipgram_50_extra_layer.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram(corpus,window_size,V):\n",
    "        loss += skipgram_50_extra_layer.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 150\n",
    "#create Skipgram model\n",
    "skipgram_150_extra_layer = Sequential()\n",
    "skipgram_150_extra_layer.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_150_extra_layer.add(Reshape((dim, )))\n",
    "skipgram_150_extra_layer.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='selu'))\n",
    "skipgram_150_extra_layer.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "#define loss function for Skipgram\n",
    "skipgram_150_extra_layer.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram(corpus,window_size,V):\n",
    "        loss += skipgram_150_extra_layer.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "#create Skipgram model\n",
    "skipgram_300_extra_layer = Sequential()\n",
    "skipgram_300_extra_layer.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram_300_extra_layer.add(Reshape((dim, )))\n",
    "skipgram_300_extra_layer.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='selu'))\n",
    "skipgram_300_extra_layer.add(Dense(input_dim=dim, units=V, kernel_initializer='uniform', activation='softmax'))\n",
    "#define loss function for Skipgram\n",
    "skipgram_300_extra_layer.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "#train Skipgram model\n",
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data_skipgram(corpus,window_size,V):\n",
    "        loss += skipgram_300_extra_layer.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def anology_m(model, word_one, word_two, word_three):\n",
    "    keyvector = model.get_weights()[0]\n",
    "    index_one = tokenizer.word_index[word_one]\n",
    "    index_two = tokenizer.word_index[word_two]\n",
    "    index_three = tokenizer.word_index[word_three]\n",
    "    result = (keyvector[index_two] - keyvector[index_one]) + keyvector[index_three]\n",
    "    \n",
    "    closest_words = {}\n",
    "    closest_word = \"\"\n",
    "    min_score = -1\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if(index not in [index_one, index_two, index_three]):\n",
    "            sim_score = cosine_similarity([keyvector[index], result])[0][1]\n",
    "            closest_words[word] = sim_score\n",
    "            if(sim_score > min_score):\n",
    "                min_score = sim_score\n",
    "                closest_word = word\n",
    "    sorted_x = sorted(closest_words.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dict(sorted_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Happy not in dataset hence we catch keyerror\n",
    "def test_acc(model):\n",
    "    hit_1 = 0\n",
    "    hit_5 = 0\n",
    "    hit_10 = 0\n",
    "    file = open('analogy_alice.txt')\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            tokens = line.strip().split(' ')\n",
    "            result = anology_m(model, tokens[0], tokens[1], tokens[2])\n",
    "            if(tokens[3] == result[0].keys()):\n",
    "                hit_1 += 1\n",
    "            elif(tokens[3] in result[:5].keys()):\n",
    "                hit_5 += 1\n",
    "            elif(tokens[3] in result.keys()):\n",
    "                hit_10 += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return hit_1, hit_5, hit_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cbow without extra layer results\n",
    "res_cbow_50_1, res_cbow_50_2, res_cbow_50_3 = test_acc(cbow_model_50)\n",
    "res_cbow_150_1, res_cbow_150_2, res_cbow_150_3 = test_acc(cbow_model_150)\n",
    "res_cbow_300_1, res_cbow_300_2, res_cbow_300_3 = test_acc(cbow_model_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(res_cbow_300_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#skipgram without extra layer results\n",
    "res_skipgram_50_1, res_skipgram_50_2, res_skipgram_50_3 = test_acc(skipgram_50)\n",
    "res_skipgram_150_1, res_skipgram_150_2, res_skipgram_150_3 = test_acc(skipgram_150)\n",
    "res_skipgram_300_1, res_skipgram_300_2, res_skipgram_300_3 = test_acc(skipgram_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(res_skipgram_300_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cbow wextra layer results\n",
    "res_cbow_el_50_1, res_cbow_el_50_2, res_cbow_el_50_3 = test_acc(cbow_model_50_extra_layer)\n",
    "res_cbow_el_150_1, res_cbow_el_150_2, res_cbow_el_150_3 = test_acc(cbow_model_150_extra_layer)\n",
    "res_cbow_el_300_1, res_cbow_el_300_2, res_cbow_el_300_3 = test_acc(cbow_model_300_extra_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skipgram with extra layer results\n",
    "res_skipgram_el_50_1, res_skipgram_el_50_2, res_skipgram_el_50_3 = test_acc(skipgram_50_extra_layer)\n",
    "res_skipgram_el_150_1, res_skipgram_el_150_2, res_skipgram_el_150_3 = test_acc(skipgram_150_extra_layer)\n",
    "res_skipgram_el_300_1, res_skipgram_el_300_2, res_skipgram_el_300_3 = test_acc(skipgram_300_extra_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization results trained word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the trained word embeddings with the word-word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    \n",
    "    plt.figure(figsize=(14,14))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                    xy=(x[i], y[i]),\n",
    "                    xytext=(5,2),\n",
    "                    textcoords='offset points',\n",
    "                    ha='right',\n",
    "                    va='bottom')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained word embeddings of word2vec\n",
    "\n",
    "path_word2vec = \"C:/Users/Niek/Documents/school/RedSystems/data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path_word2vec, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretraind word embeddings of Glove\n",
    "\n",
    "path = \"your path /glove.6B/glove.6B.300d_converted.txt\"\n",
    "\n",
    "#convert GloVe into word2vec format\n",
    "gensim.scripts.glove2word2vec.get_glove_info(path)\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(path, \"glove_converted.txt\")\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance with your own trained word embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
